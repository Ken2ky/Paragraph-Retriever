{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Stuff"
      ],
      "metadata": {
        "id": "IaF3491xCBtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing"
      ],
      "metadata": {
        "id": "mSSyYba1e9tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz\n",
        "!pip install pymupdf\n",
        "!pip install path\n",
        "!pip install binarytree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_bXGMIgTgX-",
        "outputId": "be0b7030-343c-464c-fa78-45c38f7787d9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.8/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.8/dist-packages (from fitz) (5.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fitz) (1.22.4)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.8/dist-packages (from fitz) (0.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fitz) (1.10.1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (from fitz) (3.0.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.8/dist-packages (from fitz) (1.5)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.8/dist-packages (from fitz) (5.3.0)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.8/dist-packages (from fitz) (1.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fitz) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from configobj->fitz) (1.15.0)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (3.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (3.9.0)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (6.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (3.0)\n",
            "Requirement already satisfied: traits!=5.0,<6.4,>=4.6 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (2.0.0)\n",
            "Requirement already satisfied: looseversion in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (23.0)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (8.1.3)\n",
            "Requirement already satisfied: etelemetry>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fitz) (2022.7.1)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (0.16.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (2.25.1)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (4.9.2)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.8/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot>=1.2.3->nipype->fitz) (3.0.9)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib>=5.0.0->nipype->fitz) (67.5.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->pyxnat->fitz) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->pyxnat->fitz) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->pyxnat->fitz) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->pyxnat->fitz) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.8/dist-packages (1.21.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: path in /usr/local/lib/python3.8/dist-packages (16.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: binarytree in /usr/local/lib/python3.8/dist-packages (6.5.1)\n",
            "Requirement already satisfied: setuptools-scm[toml]>=5.0.1 in /usr/local/lib/python3.8/dist-packages (from binarytree) (7.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from binarytree) (0.10.1)\n",
            "Requirement already satisfied: setuptools>=60.8.2 in /usr/local/lib/python3.8/dist-packages (from binarytree) (67.5.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from setuptools-scm[toml]>=5.0.1->binarytree) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from setuptools-scm[toml]>=5.0.1->binarytree) (23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from setuptools-scm[toml]>=5.0.1->binarytree) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.8/dist-packages (0.10.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from bert-extractive-summarizer) (4.26.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from bert-extractive-summarizer) (1.2.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (from bert-extractive-summarizer) (3.4.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (8.1.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (2.25.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (2.4.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (1.10.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy->bert-extractive-summarizer) (67.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->bert-extractive-summarizer) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers->bert-extractive-summarizer) (0.12.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->bert-extractive-summarizer) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->bert-extractive-summarizer) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->bert-extractive-summarizer) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing"
      ],
      "metadata": {
        "id": "KVc8EIooe_zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qdwyw5e1FZ_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c1a199-ca52-4596-eadd-00cd01929f48"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('words')\n",
        "nltk.download(\"popular\")\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "AufRlW_Mpc8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4cc5e17-5ae5-432a-d01e-fea8ed4ed4f5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from path import Path\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import OrderedDict\n",
        "from binarytree import Node"
      ],
      "metadata": {
        "id": "5xMOM9PXTc0R"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Textract (https://textract.readthedocs.io/en/stable/) to extract text from the PDF files. Use NLTK to tokenize them and the Porter's Stemmer to stem the tokens. Use arrays instead of lists since we have a fixed corpus. If time permits, maybe implement wildcards and a ranking system?"
      ],
      "metadata": {
        "id": "oGV0PJ5dDMxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Path and Postings List Initialization"
      ],
      "metadata": {
        "id": "1JxfoA_JDmA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty dictionary for the postings list\n",
        "postings = {}\n",
        "# initialize the PorterStemmer and stopwords\n",
        "ps = nltk.wordnet.WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "F5m5zqI1qm7z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pth = Path('/content/drive/Othercomputers/My Laptop (1)/Academics/3-2/CS F469 Information Retrieval/Assignment-1_2/SamplePolicyDocs')"
      ],
      "metadata": {
        "id": "-42gip78SpC0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc = fitz.open('/content/drive/Othercomputers/My Laptop (1)/Academics/3-2/CS F469 Information Retrieval/Assignment-1_2/SamplePolicyDocs/Auto/7thEditionPolicy.pdf')"
      ],
      "metadata": {
        "id": "EZ02IjsXTMBU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summarizer"
      ],
      "metadata": {
        "id": "ouYVcbcz13zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def summarize(text: str, summary_length: int = 5) -> str:\n",
        "    # Tokenize the text into sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word.lower() for word in word_tokenize(text)]\n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "    # Create a frequency table for the words\n",
        "    word_freq = dict()\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "    # Compute the sentence scores based on word frequencies\n",
        "    sentence_scores = dict()\n",
        "    for sentence in sentences:\n",
        "        sentence_words = [word.lower() for word in word_tokenize(sentence)]\n",
        "        sentence_score = sum([word_freq.get(word, 0) for word in sentence_words])\n",
        "        sentence_scores[sentence] = sentence_score\n",
        "    # Select the top N sentences with the highest scores\n",
        "    summary_sentences = heapq.nlargest(summary_length, sentence_scores, key=sentence_scores.get)\n",
        "    # Return the summary as a single string\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuXbzTx915MO",
        "outputId": "48e08f37-8c59-4547-e171-a06aa18e887d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spelling Corrector"
      ],
      "metadata": {
        "id": "NS8vi1gv_v_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import words\n",
        "correct_words = words.words()"
      ],
      "metadata": {
        "id": "3UnHxCGFAtG6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(correct_words)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def edits3(word):\n",
        "    \"All edits that are three edits away from `word`.\"\n",
        "    return (e3 for e2 in edits2(word) for e3 in edits1(e2))\n",
        "\n",
        "def known_edits2(word):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS and are two edits away from `word`.\"\n",
        "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in WORDS)\n",
        "\n",
        "def known_edits3(word):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS and are three edits away from `word`.\"\n",
        "    return set(e3 for e2 in edits2(word) for e3 in edits1(e2) if e3 in WORDS)\n",
        "\n",
        "def candidates_v2(word):\n",
        "    \"Generate possible spelling corrections for word with edits up to 3.\"\n",
        "    return (known([word]) or known(edits1(word)) or known_edits2(word) or known_edits3(word) or [word])\n",
        "\n",
        "def correction_v2(word): \n",
        "    \"Most probable spelling correction for word with edits up to 3.\"\n",
        "    return max(candidates_v2(word), key=P)"
      ],
      "metadata": {
        "id": "JvP2bMTk_yMt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Acronym Expansion using Beautiful Soup"
      ],
      "metadata": {
        "id": "3HIEgpMaNlRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def acronym_expander(acronym):\n",
        "    url = f'https://www.acronymfinder.com/{acronym}.html'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    full_forms_text = soup.find_all('a', {'class': 'r5'})\n",
        "    titles = []\n",
        "    for i in full_forms_text:\n",
        "      titles.append(i['title'])\n",
        "    if len(titles) == 0:\n",
        "        return None\n",
        "    for i in range(0,len(titles)):\n",
        "      titles[i] = titles[i].split('- ')[1]\n",
        "    return titles\n",
        "# Example usage\n",
        "print(acronym_expander('defn'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsOEiX1XNpOt",
        "outputId": "a33957c9-854c-4c12-a4fe-cb096fb9bc69"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Deficiency', 'Definition']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating the Postings List"
      ],
      "metadata": {
        "id": "hlU5TXalzrzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_dir=Path('SamplePolicyDocs/Auto/')\n",
        "# loop through each PDF file in the directory\n",
        "for pdf_path in pdf_dir.glob(\"*.pdf\"):\n",
        "    # open the PDF file with fitz\n",
        "    doc = fitz.open(str(pdf_path))\n",
        "    for page_num, page in enumerate(doc):\n",
        "        # extract the text blocks from the page\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "        # loop through each text block in the page\n",
        "        for block_num, block in enumerate(blocks):\n",
        "            # tokenize the text block\n",
        "            tokens = word_tokenize(block[4])\n",
        "            # stem the tokens\n",
        "            stemmed_tokens = [ps.lemmatize(token) for token in tokens]\n",
        "            # loop through each stemmed token in the text block\n",
        "            for position, token in enumerate(stemmed_tokens):\n",
        "                # create the document ID tuple\n",
        "                doc_id = (pdf_path.stem.split('.')[0], page_num, block_num)\n",
        "                # add the term and document ID to the postings list\n",
        "                if token not in postings:\n",
        "                    # create a tuple with the document IDs, frequency of the term, and positions\n",
        "                    postings[token] = ([doc_id], 1, [position])\n",
        "                else:\n",
        "                    # if the document ID is already in the postings list\n",
        "                    if doc_id in postings[token][0]:\n",
        "                        # increment the frequency of the term and add the new position\n",
        "                        freq = postings[token][1]\n",
        "                        positions = postings[token][2]\n",
        "                        positions.append(position)\n",
        "                        postings[token] = (postings[token][0], freq+1, positions)\n",
        "                    # if the document ID is not in the postings list\n",
        "                    else:\n",
        "                        # add the document ID, frequency of the term, and position\n",
        "                        postings[token] = (postings[token][0]+[doc_id], postings[token][1]+1, postings[token][2]+[position])\n",
        "\n",
        "# sort the dictionary for easier searching\n",
        "postings = OrderedDict(sorted(postings.items()))"
      ],
      "metadata": {
        "id": "tRnLfvcxIgBn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for term, posting in postings.items():\n",
        "#     print(term + \":\")\n",
        "#     print(\"\\tDocument IDs:\", posting[0])\n",
        "#     print(\"\\tFrequency:\", posting[1])"
      ],
      "metadata": {
        "id": "KkWV_E-gA5YP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_retrieval(query, postings):\n",
        "    # tokenize the query string\n",
        "    words = query.split()\n",
        "    final=\"\"\n",
        "    for word in words:\n",
        "      if(correction_v2(word)!=word and word!=\"AND\" and word!=\"OR\" and word!=\"NOT\"):\n",
        "        print(\"Did you mean: \"+word+\":\"+correction_v2(word))\n",
        "        input3=input(\"yes or no \\n\")\n",
        "        if(input3==\"yes\"):\n",
        "          word=correction_v2(word)\n",
        "      final=final+word+\" \"\n",
        "    tokens = word_tokenize(final)\n",
        "    for i in range(0,len(tokens)):\n",
        "      if tokens[i] != \"NOT\" and tokens[i] != \"OR\" and tokens[i] != \"AND\":\n",
        "        tokens[i] = ps.lemmatize(tokens[i])\n",
        "      else:\n",
        "        continue\n",
        "    # initialize result set\n",
        "    result = set()\n",
        "    # iterate over tokens in query\n",
        "    for token in tokens:\n",
        "        # check if token is a NOT operand\n",
        "        if token.startswith('NOT'):\n",
        "            # remove the NOT operand from the token\n",
        "            term = token[3:]\n",
        "            # get the document IDs that contain the term\n",
        "            docs = set(postings.get(term, ([], 0, []))[0])\n",
        "            # remove the document IDs from the result set\n",
        "            result.difference_update(docs)\n",
        "        # check if token is an AND operand\n",
        "        elif token == 'AND':\n",
        "            # skip to next token\n",
        "            continue\n",
        "        # check if token is an OR operand\n",
        "        elif token == 'OR':\n",
        "            # skip to next token\n",
        "            continue\n",
        "        # token is a term\n",
        "        else:\n",
        "            # get the document IDs that contain the term\n",
        "            docs = set(postings.get(token, ([], 0, []))[0])\n",
        "            # add the document IDs to the result set\n",
        "            result.update(docs)\n",
        "    # iterate over tokens in query again\n",
        "    for i, token in enumerate(tokens):\n",
        "        # check if token is an AND operand\n",
        "        if token == 'AND':\n",
        "            # get the document IDs that match the next term in the query\n",
        "            next_docs = set(postings.get(tokens[i+1], ([], 0, []))[0])\n",
        "            # intersect the result set with the next set of document IDs\n",
        "            result.intersection_update(next_docs)\n",
        "        # check if token is an OR operand\n",
        "        elif token == 'OR':\n",
        "            # get the document IDs that match the next term in the query\n",
        "            next_docs = set(postings.get(tokens[i+1], ([], 0, []))[0])\n",
        "            # union the result set with the next set of document IDs\n",
        "            result.update(next_docs)\n",
        "    # return the result set\n",
        "    return result"
      ],
      "metadata": {
        "id": "Skv69q7UIq_V"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_query_res(query):\n",
        "  result = boolean_retrieval(query, postings)\n",
        "  f.write(\"\\nRetrieved Documents: {}\".format(len(result)))\n",
        "  print(\"\\nRetrieved Documents: {}\".format(len(result)))\n",
        "  for r in result:\n",
        "    if r[0] == '1215E':\n",
        "      doc = fitz.open('SamplePolicyDocs/Auto/'+r[0]+'.2.pdf')\n",
        "    else:\n",
        "      doc = fitz.open('SamplePolicyDocs/Auto/'+r[0]+'.pdf')\n",
        "    pg = doc[r[1]]\n",
        "    bl = pg.get_text(\"blocks\")\n",
        "    print(\"\\nFile Path: {}\".format(str(doc)))\n",
        "    print(\"Page Number: {}\".format(int(str(pg).split(' ')[1]) + 1))\n",
        "    print(bl[r[2]][4])\n",
        "    print(\"A short summary: \"+ summarize(bl[r[2]][4]))\n",
        "    f.write(\"\\nFile Path: {}\".format(str(doc)))\n",
        "    f.write(\"Page Number: {}\".format(int(str(pg).split(' ')[1]) + 1))\n",
        "    f.write(bl[r[2]][4])\n",
        "    f.write(\"A short summary: \"+ summarize(bl[r[2]][4]))"
      ],
      "metadata": {
        "id": "EgXcUq4YyBAw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print_query_res(\"paid AND your AND premium\")"
      ],
      "metadata": {
        "id": "dw0ChklE4zSb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Binary Tree Implementation"
      ],
      "metadata": {
        "id": "-rT7vuplx4wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, char: str):\n",
        "        self.char = char\n",
        "        self.children = []\n",
        "        self.postings = []\n",
        "        \n",
        "class BinaryTree:\n",
        "    def __init__(self, postings):\n",
        "        self.root = Node('')\n",
        "        self.build_tree(postings)\n",
        "        \n",
        "    def build_tree(self, postings):\n",
        "        for term, (doc_ids, freq, pos) in postings.items():\n",
        "            current_node = self.root\n",
        "            for char in term:\n",
        "                child_node = None\n",
        "                for child in current_node.children:\n",
        "                    if child.char == char:\n",
        "                        child_node = child\n",
        "                        break\n",
        "                if child_node is None:\n",
        "                    child_node = Node(char)\n",
        "                    current_node.children.append(child_node)\n",
        "                current_node = child_node\n",
        "            current_node.postings.append((term, doc_ids, freq, pos))\n",
        "            \n",
        "    def search(self, query: str):\n",
        "        results = []\n",
        "        query_node = self.root\n",
        "        for char in query:\n",
        "            if char == '*':\n",
        "                results += self.traverse(query_node)\n",
        "                break\n",
        "            else:\n",
        "                child_node = None\n",
        "                for child in query_node.children:\n",
        "                    if child.char == char:\n",
        "                        child_node = child\n",
        "                        break\n",
        "                if child_node is None:\n",
        "                    return []\n",
        "                query_node = child_node\n",
        "        results += query_node.postings\n",
        "        return results\n",
        "    \n",
        "    def traverse(self, node: Node):\n",
        "        results = []\n",
        "        if len(node.postings) > 0:\n",
        "            results += node.postings\n",
        "        for child in node.children:\n",
        "            results += self.traverse(child)\n",
        "        return results"
      ],
      "metadata": {
        "id": "4BDAb5zEwv7Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reverse Binary Tree Implementation"
      ],
      "metadata": {
        "id": "4rvWzUimx7zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseNode:\n",
        "    def __init__(self, char: str):\n",
        "        self.char = char\n",
        "        self.children = []\n",
        "        self.postings = []\n",
        "        \n",
        "class ReverseBinaryTree:\n",
        "    def __init__(self, postings):\n",
        "        self.root = ReverseNode('')\n",
        "        self.build_tree(postings)\n",
        "        \n",
        "    def build_tree(self, postings):\n",
        "        for term, (doc_ids, freq, pos) in postings.items():\n",
        "            current_node = self.root\n",
        "            for char in reversed(term):\n",
        "                child_node = None\n",
        "                for child in current_node.children:\n",
        "                    if child.char == char:\n",
        "                        child_node = child\n",
        "                        break\n",
        "                if child_node is None:\n",
        "                    child_node = ReverseNode(char)\n",
        "                    current_node.children.append(child_node)\n",
        "                current_node = child_node\n",
        "            current_node.postings.append((term, doc_ids, freq, pos))\n",
        "            \n",
        "    def search(self, query: str):\n",
        "        results = []\n",
        "        query_node = self.root\n",
        "        for char in reversed(query):\n",
        "            if char == '*':\n",
        "                results += self.traverse(query_node)\n",
        "                break\n",
        "            else:\n",
        "                child_node = None\n",
        "                for child in query_node.children:\n",
        "                    if child.char == char:\n",
        "                        child_node = child\n",
        "                        break\n",
        "                if child_node is None:\n",
        "                    return []\n",
        "                query_node = child_node\n",
        "        results += query_node.postings\n",
        "        return results\n",
        "    \n",
        "    def traverse(self, node: ReverseNode):\n",
        "        results = []\n",
        "        if len(node.postings) > 0:\n",
        "            results += node.postings\n",
        "        for child in node.children:\n",
        "            results += self.traverse(child)\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "luiPjcIdx3x_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tree = ReverseBinaryTree(postings)\n",
        "# results1 = tree.search(\"*at\")\n",
        "# result = []\n",
        "# for i in range(0, len(results1)):\n",
        "#   #print(results1[i])\n",
        "#   for j in results1[i][1]:\n",
        "#     result.append(j)\n",
        "# print(set(result))"
      ],
      "metadata": {
        "id": "YF4owN7plI57"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Search"
      ],
      "metadata": {
        "id": "xPNjj2bdyFl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, make the posting list by linking every distinct term with the document index or ptr in the tokens array(in this case, the block of text) as discussed in class. Clean up the token array to remove empty arrays and stop words."
      ],
      "metadata": {
        "id": "8uASWVMytmmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(list1, list2):\n",
        "    intersection_list = []\n",
        "    for element in list1:\n",
        "        if element in list2 and element not in intersection_list:\n",
        "            intersection_list.append(element)\n",
        "    return intersection_list\n",
        "from collections import defaultdict\n",
        "\n",
        "def phrase_query(query, postings):\n",
        "    # Tokenize the query and stem the tokens\n",
        "    tokens = word_tokenize(query)\n",
        "    query_tokens = [ps.lemmatize(token) for token in tokens]\n",
        "    #print(query_tokens)\n",
        "    # Create a dictionary to store the positions of each query term\n",
        "    term_positions = defaultdict(list)\n",
        "    for i, token in enumerate(query_tokens):\n",
        "        if token in postings:\n",
        "            term_positions[token].extend(postings[token][2])\n",
        "    try:\n",
        "    # Find the positions where the terms occur together in each document\n",
        "      candidate_docs = set(postings[query_tokens[0]][0])\n",
        "      for token in query_tokens[1:]:\n",
        "          if token in postings:\n",
        "              candidate_docs = candidate_docs.intersection(postings[token][0])\n",
        "    except KeyError:\n",
        "      return -1\n",
        "    \n",
        "    candidate_docs = sorted(candidate_docs)\n",
        "    #print(candidate_docs)\n",
        "    return candidate_docs\n",
        "\n",
        "def print_phrase_query_res(query):\n",
        "  if phrase_query(query, postings) == -1:\n",
        "    print(\"Not Found.\")\n",
        "    return\n",
        "  else:\n",
        "    result = phrase_query(query, postings)\n",
        "    print(\"\\nRetrieved Documents: {}\".format(len(result)))\n",
        "    f.write(\"\\nRetrieved Documents: {}\".format(len(result)))\n",
        "    for r in result:\n",
        "      if r[0] == '1215E':\n",
        "        doc = fitz.open('SamplePolicyDocs/Auto/'+r[0]+'.2.pdf')\n",
        "      else:\n",
        "        doc = fitz.open('SamplePolicyDocs/Auto/'+r[0]+'.pdf')\n",
        "      pg = doc[r[1]]\n",
        "      bl = pg.get_text(\"blocks\")\n",
        "      print(\"\\nFile Path: {}\".format(str(doc)))\n",
        "      print(\"Page Number: {}\".format(int(str(pg).split(' ')[1]) + 1))\n",
        "      print(bl[r[2]][4])\n",
        "      print(\"A short summary: \"+ summarize(bl[r[2]][4]))\n",
        "      f.write(\"\\nFile Path: {}\".format(str(doc)))\n",
        "      f.write(\"Page Number: {}\".format(int(str(pg).split(' ')[1]) + 1))\n",
        "      f.write(bl[r[2]][4])\n",
        "      f.write(\"A short summary: \"+ summarize(bl[r[2]][4]))\n",
        "        \n",
        "def print_wildcard_query_res(result):\n",
        "    print(\"\\nRetrieved Documents: {}\".format(len(result)))\n",
        "    f.write(\"\\nRetrieved Documents: {}\".format(len(result)))\n",
        "    for r in result:\n",
        "      if r[0] == '1215E':\n",
        "        doc = fitz.open('SamplePolicyDocs/Auto/'+r[0]+'.2.pdf')\n",
        "      else:\n",
        "        doc = fitz.open('SamplePolicyDocs/Auto/'+r[0]+'.pdf')\n",
        "      pg = doc[r[1]]\n",
        "      bl = pg.get_text(\"blocks\")\n",
        "      print(\"\\nFile Path: {}\".format(str(doc)))\n",
        "      print(\"Page Number: {}\".format(int(str(pg).split(' ')[1]) + 1))\n",
        "      print(bl[r[2]][4])\n",
        "      print(\"A short summary: \"+ summarize(bl[r[2]][4]))\n",
        "      f.write(\"\\nFile Path: {}\".format(str(doc)))\n",
        "      f.write(\"Page Number: {}\".format(int(str(pg).split(' ')[1]) + 1))\n",
        "      f.write(bl[r[2]][4])\n",
        "      f.write(\"A short summary: \"+ summarize(bl[r[2]][4]))\n",
        "\n",
        "user_input=input(\"Enter a number (1-3)\"+\"\\n\"+\"1. Wildcard\"+\"\\n\"+\"2. Phrase Query\"+\"\\n\"+\"3. Boolean Query\"+\"\\n\")\n",
        "f = open(\"results.txt\", \"w\")\n",
        "if user_input == \"1\":\n",
        "    input1=input(\"Enter query:\")\n",
        "    s1=\"\"\n",
        "    s2=\"*\"\n",
        "    flag= False\n",
        "    for char in input1:\n",
        "        if char == '*':\n",
        "            flag= True\n",
        "        elif flag:\n",
        "            s2+=char\n",
        "        else:\n",
        "            s1+=char\n",
        "    s1+='*'\n",
        "    \n",
        "    if(s1!=\"*\"):\n",
        "        tree = BinaryTree(postings)\n",
        "        results1 = tree.search(s1)\n",
        "    if(s2!=\"*\"):\n",
        "        rev_tree = ReverseBinaryTree(postings)\n",
        "        results2 = rev_tree.search(s2)\n",
        "    if(s1==\"*\" and s2!=\"*\"):\n",
        "        results=results2\n",
        "        result = []\n",
        "        for i in range(0, len(results)):\n",
        "          #print(results1[i])\n",
        "          for j in results[i][1]:\n",
        "            result.append(j)\n",
        "        print_wildcard_query_res(set(result))\n",
        "        # for term, doc_ids, freq, pos in results:\n",
        "        #   print(f\"{term}: {doc_ids}, {freq},{pos}\")\n",
        "    elif(s2==\"*\" and s1!=\"*\"):\n",
        "        results=results1\n",
        "        result = []\n",
        "        for i in range(0, len(results)):\n",
        "        #print(results1[i])\n",
        "          for j in results[i][1]:\n",
        "            result.append(j)\n",
        "        print_wildcard_query_res(set(result))\n",
        "        # for term, doc_ids, freq, pos in results:\n",
        "        #   print(f\"{term}: {doc_ids}, {freq},{pos}\")\n",
        "    elif(s1!=\"*\" and s2!=\"*\"):\n",
        "        results=intersection(results1,results2)\n",
        "        result = []\n",
        "        for i in range(0, len(results)):\n",
        "        #print(results1[i])\n",
        "          for j in results[i][1]:\n",
        "            result.append(j)\n",
        "        print_wildcard_query_res(set(result))\n",
        "        # for term, doc_ids, freq, pos in results:\n",
        "        #   print(f\"{term}: {doc_ids}, {freq},{pos}\")\n",
        "    else:\n",
        "        print(\"Invalid input\")\n",
        "    \n",
        "    \n",
        "elif user_input == \"2\":\n",
        "    input1=input(\"Enter query:\")\n",
        "    words = input1.split()\n",
        "    final=\"\"\n",
        "    final1=\"\"\n",
        "    for word in words:\n",
        "      if(acronym_expander(word)==None):\n",
        "        if(correction_v2(word)!=word):\n",
        "          print(\"Did you mean: \"+word+\":\"+correction_v2(word))\n",
        "          input3=input(\"yes or no \\n\")\n",
        "          if(input3==\"yes\"):\n",
        "            word=correction_v2(word)\n",
        "        final1=final1+word+\" \"\n",
        "        final=final+word+\" \"\n",
        "      else:\n",
        "        print(\"For \"+word+\" did you mean:\")\n",
        "        l=acronym_expander(word)\n",
        "        k=len(l)\n",
        "        if(k>5):\n",
        "          k=5\n",
        "        for i in range(0,k):\n",
        "          print(i+1)\n",
        "          print(l[i])\n",
        "        input2=input(\"Enter the index of the expansion if it matches else enter 0.\\n\")\n",
        "        if(input2==\"0\"):\n",
        "          input2=word\n",
        "        else:\n",
        "          input2=l[int(input2)-1]\n",
        "        final=final+input2+\" \"\n",
        "        final1=final1+word+\" \"\n",
        "    ##print(final1, final)\n",
        "    print_phrase_query_res(final1)\n",
        "    print_phrase_query_res(final)\n",
        "elif user_input ==\"3\":\n",
        "    input1=input(\"Enter query:\")\n",
        "    print_query_res(input1)\n",
        "else:\n",
        "    print(\"Invalid input\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIDEX_jW1Mwe",
        "outputId": "7549d870-3b7f-46bc-f52d-d84cfb7010c2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a number (1-3)\n",
            "1. Wildcard\n",
            "2. Phrase Query\n",
            "3. Boolean Query\n",
            "2\n",
            "Enter query:paid your premium\n",
            "For paid did you mean:\n",
            "1\n",
            "Preserve the American Dream\n",
            "Enter the index of the expansion if it matches else enter 0.\n",
            "0\n",
            "For premium did you mean:\n",
            "1\n",
            "Home Box Office (premium movie channel on USA cable systems)\n",
            "2\n",
            "Fast Track (instant picture of premium/loss information in the casualty industry)\n",
            "3\n",
            "Cinemax (premium cable channel)\n",
            "4\n",
            "Premium Processing (US Department of Homeland Security)\n",
            "5\n",
            "Premium Only Plan (employee benefit plan)\n",
            "Enter the index of the expansion if it matches else enter 0.\n",
            "0\n",
            "\n",
            "Retrieved Documents: 2\n",
            "\n",
            "File Path: Document('/content/drive/Othercomputers/My Laptop (1)/Academics/3-2/CS F469 Information Retrieval/Assignment-1_2/SamplePolicyDocs/Auto/7thEditionPolicy.pdf')\n",
            "Page Number: 9\n",
            "When you purchased this Part you were given the choice of either\n",
            "excluding yourself, or yourself and household members, from some\n",
            "or all of the PIP coverage. The portion of each claim you may have\n",
            "agreed not to be covered for is called a deductible. You paid a\n",
            "smaller premium if you chose a deductible. In that case, we will only\n",
            "pay up to the difference between $8,000 and the amount of your\n",
            "deductible. The deductible is shown on the Coverage Selections\n",
            "Page.\n",
            "\n",
            "A short summary: The portion of each claim you may have\n",
            "agreed not to be covered for is called a deductible. You paid a\n",
            "smaller premium if you chose a deductible. When you purchased this Part you were given the choice of either\n",
            "excluding yourself, or yourself and household members, from some\n",
            "or all of the PIP coverage. The deductible is shown on the Coverage Selections\n",
            "Page. In that case, we will only\n",
            "pay up to the difference between $8,000 and the amount of your\n",
            "deductible.\n",
            "\n",
            "File Path: Document('/content/drive/Othercomputers/My Laptop (1)/Academics/3-2/CS F469 Information Retrieval/Assignment-1_2/SamplePolicyDocs/Auto/7thEditionPolicy.pdf')\n",
            "Page Number: 31\n",
            "1. You have not paid your premium on this policy.\n",
            "\n",
            "A short summary: You have not paid your premium on this policy. 1.\n",
            "\n",
            "Retrieved Documents: 2\n",
            "\n",
            "File Path: Document('/content/drive/Othercomputers/My Laptop (1)/Academics/3-2/CS F469 Information Retrieval/Assignment-1_2/SamplePolicyDocs/Auto/7thEditionPolicy.pdf')\n",
            "Page Number: 9\n",
            "When you purchased this Part you were given the choice of either\n",
            "excluding yourself, or yourself and household members, from some\n",
            "or all of the PIP coverage. The portion of each claim you may have\n",
            "agreed not to be covered for is called a deductible. You paid a\n",
            "smaller premium if you chose a deductible. In that case, we will only\n",
            "pay up to the difference between $8,000 and the amount of your\n",
            "deductible. The deductible is shown on the Coverage Selections\n",
            "Page.\n",
            "\n",
            "A short summary: The portion of each claim you may have\n",
            "agreed not to be covered for is called a deductible. You paid a\n",
            "smaller premium if you chose a deductible. When you purchased this Part you were given the choice of either\n",
            "excluding yourself, or yourself and household members, from some\n",
            "or all of the PIP coverage. The deductible is shown on the Coverage Selections\n",
            "Page. In that case, we will only\n",
            "pay up to the difference between $8,000 and the amount of your\n",
            "deductible.\n",
            "\n",
            "File Path: Document('/content/drive/Othercomputers/My Laptop (1)/Academics/3-2/CS F469 Information Retrieval/Assignment-1_2/SamplePolicyDocs/Auto/7thEditionPolicy.pdf')\n",
            "Page Number: 31\n",
            "1. You have not paid your premium on this policy.\n",
            "\n",
            "A short summary: You have not paid your premium on this policy. 1.\n"
          ]
        }
      ]
    }
  ]
}